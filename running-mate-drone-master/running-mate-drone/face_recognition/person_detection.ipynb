{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4622b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "img = cv2.imread('/home/piai/AI_project/js.jpg')\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b02f2d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/piai/AI_project\r\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "# /home/sb020518/DLCV/Detection/yolo\n",
    "\n",
    "import os\n",
    "#가급적 절대 경로 사용. \n",
    "CUR_DIR = os.path.abspath('.')\n",
    "weights_path = os.path.join(CUR_DIR, '/home/piai/AI_project/yolov3.weights')\n",
    "config_path =  os.path.join(CUR_DIR, '/home/piai/AI_project/yolov3.cfg')\n",
    "#config 파일 인자가 먼저 옴. \n",
    "cv_net_yolo = cv2.dnn.readNetFromDarknet(config_path, weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00e7a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_names_seq = {0:'person',1:'bicycle',2:'car',3:'motorbike',4:'aeroplane',5:'bus',6:'train',7:'truck',8:'boat',9:'traffic light',10:'fire hydrant',\n",
    "                        11:'stop sign',12:'parking meter',13:'bench',14:'bird',15:'cat',16:'dog',17:'horse',18:'sheep',19:'cow',20:'elephant',\n",
    "                        21:'bear',22:'zebra',23:'giraffe',24:'backpack',25:'umbrella',26:'handbag',27:'tie',28:'suitcase',29:'frisbee',30:'skis',\n",
    "                        31:'snowboard',32:'sports ball',33:'kite',34:'baseball bat',35:'baseball glove',36:'skateboard',37:'surfboard',38:'tennis racket',39:'bottle',40:'wine glass',\n",
    "                        41:'cup',42:'fork',43:'knife',44:'spoon',45:'bowl',46:'banana',47:'apple',48:'sandwich',49:'orange',50:'broccoli',\n",
    "                        51:'carrot',52:'hot dog',53:'pizza',54:'donut',55:'cake',56:'chair',57:'sofa',58:'pottedplant',59:'bed',60:'diningtable',\n",
    "                        61:'toilet',62:'tvmonitor',63:'laptop',64:'mouse',65:'remote',66:'keyboard',67:'cell phone',68:'microwave',69:'oven',70:'toaster',\n",
    "                        71:'sink',72:'refrigerator',73:'book',74:'clock',75:'vase',76:'scissors',77:'teddy bear',78:'hair drier',79:'toothbrush' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2eb8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_names_seq = {0:'person' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "502692f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_names = {1:'person',2:'bicycle',3:'car',4:'motorcycle',5:'airplane',6:'bus',7:'train',8:'truck',9:'boat',10:'traffic light',\n",
    "                    11:'fire hydrant',12:'street sign',13:'stop sign',14:'parking meter',15:'bench',16:'bird',17:'cat',18:'dog',19:'horse',20:'sheep',\n",
    "                    21:'cow',22:'elephant',23:'bear',24:'zebra',25:'giraffe',26:'hat',27:'backpack',28:'umbrella',29:'shoe',30:'eye glasses',\n",
    "                    31:'handbag',32:'tie',33:'suitcase',34:'frisbee',35:'skis',36:'snowboard',37:'sports ball',38:'kite',39:'baseball bat',40:'baseball glove',\n",
    "                    41:'skateboard',42:'surfboard',43:'tennis racket',44:'bottle',45:'plate',46:'wine glass',47:'cup',48:'fork',49:'knife',50:'spoon',\n",
    "                    51:'bowl',52:'banana',53:'apple',54:'sandwich',55:'orange',56:'broccoli',57:'carrot',58:'hot dog',59:'pizza',60:'donut',\n",
    "                    61:'cake',62:'chair',63:'couch',64:'potted plant',65:'bed',66:'mirror',67:'dining table',68:'window',69:'desk',70:'toilet',\n",
    "                    71:'door',72:'tv',73:'laptop',74:'mouse',75:'remote',76:'keyboard',77:'cell phone',78:'microwave',79:'oven',80:'toaster',\n",
    "                    81:'sink',82:'refrigerator',83:'blender',84:'book',85:'clock',86:'vase',87:'scissors',88:'teddy bear',89:'hair drier',90:'toothbrush',\n",
    "                    91:'hair brush'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6a4fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_names = {1:'person'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2eb95f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detected_img(cv_net, img_array, conf_threshold, nms_threshold, use_copied_array=True, is_print=True):\n",
    "    global dst\n",
    "    # 원본 이미지를 네트웍에 입력시에는 (416, 416)로 resize 함. \n",
    "    # 이후 결과가 출력되면 resize된 이미지 기반으로 bounding box 위치가 예측 되므로 이를 다시 원복하기 위해 원본 이미지 shape정보 필요\n",
    "    rows = img_array.shape[0]\n",
    "    cols = img_array.shape[1]\n",
    "    draw_img = None\n",
    "    if use_copied_array:\n",
    "        draw_img = img_array.copy()\n",
    "    else:\n",
    "        draw_img = img_array\n",
    "    \n",
    "    #전체 Darknet layer에서 13x13 grid, 26x26, 52x52 grid에서 detect된 Output layer만 filtering\n",
    "    layer_names = cv_net.getLayerNames()\n",
    "    outlayer_names = [layer_names[i[0] - 1] for i in cv_net.getUnconnectedOutLayers()]\n",
    "    \n",
    "    # 로딩한 모델은 Yolov3 416 x 416 모델임. 원본 이미지 배열을 사이즈 (416, 416)으로, BGR을 RGB로 변환하여 배열 입력\n",
    "    cv_net.setInput(cv2.dnn.blobFromImage(img_array, scalefactor=1/255.0, size=(416, 416), swapRB=True, crop=False))\n",
    "    start = time.time()\n",
    "    \n",
    "    # Object Detection 수행하여 결과를 cvOut으로 반환 \n",
    "    cv_outs = cv_net.forward(outlayer_names)\n",
    "    layerOutputs = cv_net.forward(outlayer_names)\n",
    "    # bounding box의 테두리와 caption 글자색 지정\n",
    "    green_color=(0, 255, 0)\n",
    "    red_color=(0, 0, 255)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    widths=[]\n",
    "    heights=[]\n",
    "    \n",
    "    # 3개의 개별 output layer별로 Detect된 Object들에 대해서 Detection 정보 추출 및 시각화 \n",
    "    for ix, output in enumerate(cv_outs):\n",
    "        # Detected된 Object별 iteration\n",
    "        for jx, detection in enumerate(output):\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            # confidence가 지정된 conf_threshold보다 작은 값은 제외 \n",
    "            if confidence > conf_threshold:\n",
    "                #print('ix:', ix, 'jx:', jx, 'class_id', class_id, 'confidence:', confidence)\n",
    "                # detection은 scale된 좌상단, 우하단 좌표를 반환하는 것이 아니라, detection object의 중심좌표와 너비/높이를 반환\n",
    "                # 원본 이미지에 맞게 scale 적용 및 좌상단, 우하단 좌표 계산\n",
    "                center_x = int(detection[0] * cols)\n",
    "                center_y = int(detection[1] * rows)\n",
    "                width = int(detection[2] * cols)\n",
    "                height = int(detection[3] * rows)\n",
    "                left = int(center_x - width / 2)\n",
    "                top = int(center_y - height / 2)\n",
    "                # 3개의 개별 output layer별로 Detect된 Object들에 대한 class id, confidence, 좌표정보를 모두 수집\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([left, top, width, height])\n",
    "    \n",
    "    # NMS로 최종 filtering된 idxs를 이용하여 boxes, classes, confidences에서 해당하는 Object정보를 추출하고 시각화.\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "    if len(idxs) > 0:\n",
    "        for i in idxs.flatten():\n",
    "            box = boxes[i]\n",
    "            left = box[0]\n",
    "            top = box[1]\n",
    "            width = box[2]\n",
    "            height = box[3]\n",
    "            # labels_to_names 딕셔너리로 class_id값을 클래스명으로 변경. opencv에서는 class_id + 1로 매핑해야함.\n",
    "            caption = \"{}: {:.4f}\".format(labels_to_names_seq[class_ids[i]], confidences[i])\n",
    "            #cv2.rectangle()은 인자로 들어온 draw_img에 사각형을 그림. 위치 인자는 반드시 정수형.\n",
    "            cv2.rectangle(draw_img, (int(left), int(top)), (int(left+width), int(top+height)), color=green_color, thickness=2)\n",
    "            cv2.putText(draw_img, caption, (int(left), int(top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, red_color, 1)\n",
    "            dst=img_array.copy()\n",
    "            dst=img_array[top:top+height,left:left+width]\n",
    "    #cv2.imshow(dst)\n",
    "            filename2=\"/home/piai/AI_project/person.jpg\"\n",
    "\n",
    "            cv2.imwrite(filename2, dst)\n",
    "            print(left, top, width, height)\n",
    "            \n",
    "    if is_print:\n",
    "        print('Detection 수행시간:',round(time.time() - start, 2),\"초\")\n",
    "        \n",
    "    print(width, height)\n",
    "\n",
    "   \n",
    "    return draw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97405ee0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection 수행시간: 1.45 초\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'width' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-0a27874d09df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mnms_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Object Detetion 수행 후 시각화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdraw_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_detected_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_net_yolo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnms_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_copied_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_print\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mimg_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-41f358c18df4>\u001b[0m in \u001b[0;36mget_detected_img\u001b[0;34m(cv_net, img_array, conf_threshold, nms_threshold, use_copied_array, is_print)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Detection 수행시간:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"초\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'width' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# image 로드 \n",
    "img = cv2.imread('/home/piai/AI_project/js.jpg')\n",
    "\n",
    "#coco dataset 클래스명 매핑\n",
    "\n",
    "import os\n",
    "#가급적 절대 경로 사용. \n",
    "CUR_DIR = os.path.abspath('.')\n",
    "weights_path = os.path.join(CUR_DIR, '/home/piai/AI_project/yolov3.weights')\n",
    "config_path =  os.path.join(CUR_DIR, '/home/piai/AI_project/yolov3.cfg')\n",
    "\n",
    "# tensorflow inference 모델 로딩\n",
    "cv_net_yolo = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "    \n",
    "conf_threshold = 0.5\n",
    "nms_threshold = 0.4\n",
    "# Object Detetion 수행 후 시각화 \n",
    "draw_img = get_detected_img(cv_net_yolo, img, conf_threshold=conf_threshold, nms_threshold=nms_threshold, use_copied_array=True, is_print=True)\n",
    "\n",
    "img_rgb = cv2.cvtColor(draw_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(img_rgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36921366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e10c42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
